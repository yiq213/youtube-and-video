{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dazbo's YouTube and Video Demos - with Google Gemini\n",
    "\n",
    "## Overview\n",
    "\n",
    "Welcome to notebook #2 in this tutorial guide. This notebook follows on from [YouTube and Video Demos #1](youtube-demos.ipynb). In the previous notebook I demonstrated:\n",
    "\n",
    "- Multiple methods for downloading videos and extracting audio\n",
    "- How to transcribe audio to text using a free speech-to-text API\n",
    "- How to extract existing transcripts and translate to different languages\n",
    "\n",
    "In this part we'll strip out parts of the first notebook we don't need, and add some smarts using Google technology.\n",
    "\n",
    "## How to Launch and Run this Notebook\n",
    "\n",
    "- The source for this notebook source lives in my GitHub repo, <a href=\"https://github.com/derailed-dash/youtube-and-video\" target=\"_blank\">Youtube-and-Video</a>.\n",
    "- Check out further guidance - including tips on how to run the notebook - in the project's `README.md`.\n",
    "- For example, you could...\n",
    "  - Run the notebook locally, in your own Jupyter environment.\n",
    "  - Run the notebook in a cloud-based Jupyter environment, with no setup required on your part! For example, with **Google Colab**: <br><br><a href=\"https://colab.research.google.com/github/derailed-dash/youtube-and-video/blob/main/src/notebooks/youtube-demos-with-google-gemini.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Google Colab\"/></a><br><br>It looks like this:<br><br><img src=\"static/images/collab-view.png\" width=\"640px\"></img>\n",
    "- For more ways to run Jupyter Notebooks, check out [my guide](https://medium.com/python-in-plain-english/five-ways-to-run-jupyter-labs-and-notebooks-23209f71e5c0).\n",
    "\n",
    "**When running this notebook, first execute the cells in the [Setup](#Setup) section, as described below.** Then you can experiment with any of the subsequent cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "First, let's install any dependent packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --no-cache-dir python-dotenv \\\n",
    "                                      dazbo-commons \\\n",
    "                                      pytubefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "import dazbo_commons as dc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dazbo_commons as dc\n",
    "# Colab requires an older version of Ipykernel\n",
    "if not \"google.colab\" in sys.modules:\n",
    "    pass\n",
    "    %pip install --upgrade --no-cache-dir ipykernel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging\n",
    "\n",
    "Now we'll setup logging. Here I'm using coloured logging from my [dazbo-commons](https://pypi.org/project/dazbo-commons/) package. Feel free to change the logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m14:06:55.440:dazbo-yt-demos - INF: Logger initialised.\u001b[39m\n",
      "\u001b[34m14:06:55.443:dazbo-yt-demos - DBG: DEBUG level logging enabled.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "APP_NAME=\"dazbo-yt-demos\"\n",
    "logger = dc.retrieve_console_logger(APP_NAME)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.info(\"Logger initialised.\")\n",
    "logger.debug(\"DEBUG level logging enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Locations\n",
    "\n",
    "Here we initialise some file path locations, e.g. an output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m14:06:58.128:dazbo-yt-demos - DBG: script_name: dazbo-yt-demos\u001b[39m\n",
      "\u001b[34m14:06:58.129:dazbo-yt-demos - DBG: script_dir: /mnt/c/Users/djl/localdev/python/youtube-and-video/src/notebooks/dazbo-yt-demos\u001b[39m\n",
      "\u001b[34m14:06:58.130:dazbo-yt-demos - DBG: input_dir: /mnt/c/Users/djl/localdev/python/youtube-and-video/src/notebooks/dazbo-yt-demos/input\u001b[39m\n",
      "\u001b[34m14:06:58.130:dazbo-yt-demos - DBG: output_dir: /mnt/c/Users/djl/localdev/python/youtube-and-video/src/notebooks/dazbo-yt-demos/output\u001b[39m\n",
      "\u001b[34m14:06:58.131:dazbo-yt-demos - DBG: input_file: /mnt/c/Users/djl/localdev/python/youtube-and-video/src/notebooks/dazbo-yt-demos/input/input.txt\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "locations = dc.get_locations(APP_NAME)\n",
    "for attribute, value in vars(locations).items():\n",
    "    logger.debug(f\"{attribute}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filename(filename):\n",
    "    \"\"\" Create a clean filename by removing unallowed characters. \"\"\"\n",
    "    pattern = r'[^a-zA-Z0-9._\\s-]'\n",
    "    return  re.sub(pattern, '_', filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Videos to Work With\n",
    "\n",
    "We start by defining a list of videos to test our application with, along with a function that takes a full YouTube URL and returns just the id portion.\n",
    "\n",
    "Iâ€™ve used these videos becauseâ€¦\n",
    "\n",
    "- The first is the fantastic [Burning Bridges](https://www.youtube.com/watch?v=udRAIF6MOm8) by Sigrid. The video has no embedded transcript.\n",
    "- The second is the beautiful song [I Believe](https://www.youtube.com/watch?v=CiTn4j7gVvY) by Melissa Hollick. Itâ€™s one of my favourite songs of all time. When I get a migraine, I turn off the lights, and listen to this to feel better! And for those who enjoy gaming, this song is the end titles to the amazing Wolfenstein: New Order game. This video has an embedded transcript.\n",
    "- Then we have a short [Jim Carey speech](https://www.youtube.com/watch?v=nLgHNu2N3JU), which gives us dialog without music or other ambient noise. It has an embedded transcript.\n",
    "- And finally, a [Ukrainian song](https://www.youtube.com/watch?v=d4N82wPpdg8) from Eurovision 2024, by Jerry Heil and Alyona Alyona. This gives us an opportunity to test translation. It also has an embedded transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videos to download\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=udRAIF6MOm8\",  # Sigrid - Burning Bridges (English)\n",
    "    \"https://www.youtube.com/watch?v=CiTn4j7gVvY\",  # Melissa Hollick - I Believe (English)\n",
    "    \"https://www.youtube.com/watch?v=nLgHNu2N3JU\",  # Jim Carey - Motivational speech (English)\n",
    "    \"https://www.youtube.com/watch?v=d4N82wPpdg8\",  # Jerry Heil & Alyona Alyona - Teresa & Maria (Ukrainian)\n",
    "]\n",
    "\n",
    "def get_video_id(url: str) -> str:\n",
    "    \"\"\" Return the video ID, which is the part after 'v=' \"\"\"\n",
    "    return url.split(\"v=\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Videos and Extracting Audio\n",
    "\n",
    "Let's use the [pytubefix](https://github.com/JuanBindez/pytubefix) library to download YouTube videos, and then to download mp3 audio-only streams as files.\n",
    "\n",
    "This library is a community-maintained fork of `pytube`. It was created to provide quick fixes for issues that the official pytube library faced, particularly when YouTube's updates break `pytube`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:46:52.975:dazbo-yt-demos - INF: Downloads progress: 1/4\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:46:53.145:dazbo-yt-demos - INF: Getting: Sigrid - Burning Bridges\u001b[39m\n",
      "\u001b[32m00:47:19.145:dazbo-yt-demos - INF: Downloading video Sigrid - Burning Bridges.mp4 ...\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " â†³ |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:47:39.932:dazbo-yt-demos - INF: Creating audio...\u001b[39m\n",
      "/tmp/ipykernel_89553/3641805842.py:28: DeprecationWarning: The 'mp3' parameter is deprecated and will be removed in a future version. All audio will be downloaded as .m4a, the correct extension for the MPEG-4 AAC codec.\n",
      "  audio_stream.download(output_path=output_locn, filename=cleaned, mp3=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " â†³ |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:48:03.593:dazbo-yt-demos - INF: Done\u001b[39m\n",
      "\u001b[32m00:48:14.211:dazbo-yt-demos - INF: Downloads progress: 2/4\u001b[39m\n",
      "\u001b[32m00:48:17.390:dazbo-yt-demos - INF: Getting: Wolfenstein: The New Order - I Believe - Melissa Hollick (Official Ending Song)\u001b[39m\n",
      "\u001b[32m00:48:39.972:dazbo-yt-demos - INF: Downloading video Wolfenstein_ The New Order - I Believe - Melissa Hollick _Official Ending Song_.mp4 ...\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " â†³ |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:48:42.392:dazbo-yt-demos - INF: Creating audio...\u001b[39m\n",
      "/tmp/ipykernel_89553/3641805842.py:28: DeprecationWarning: The 'mp3' parameter is deprecated and will be removed in a future version. All audio will be downloaded as .m4a, the correct extension for the MPEG-4 AAC codec.\n",
      "  audio_stream.download(output_path=output_locn, filename=cleaned, mp3=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " â†³ |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:48:45.745:dazbo-yt-demos - INF: Done\u001b[39m\n",
      "\u001b[32m00:48:47.943:dazbo-yt-demos - INF: Downloads progress: 3/4\u001b[39m\n",
      "\u001b[32m00:48:51.097:dazbo-yt-demos - INF: Getting: Jim Carrey Commencement Speech His Father's Inspiration\u001b[39m\n",
      "\u001b[32m00:48:55.716:dazbo-yt-demos - INF: Downloading video Jim Carrey Commencement Speech His Father_s Inspiration.mp4 ...\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " â†³ |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:48:56.247:dazbo-yt-demos - INF: Creating audio...\u001b[39m\n",
      "/tmp/ipykernel_89553/3641805842.py:28: DeprecationWarning: The 'mp3' parameter is deprecated and will be removed in a future version. All audio will be downloaded as .m4a, the correct extension for the MPEG-4 AAC codec.\n",
      "  audio_stream.download(output_path=output_locn, filename=cleaned, mp3=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " â†³ |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:48:56.513:dazbo-yt-demos - INF: Done\u001b[39m\n",
      "\u001b[32m00:48:56.515:dazbo-yt-demos - INF: Downloads progress: 4/4\u001b[39m\n",
      "\u001b[32m00:48:56.671:dazbo-yt-demos - INF: Getting: alyona alyona & Jerry Heil - Teresa & Maria (LIVE) | Ukraine ðŸ‡ºðŸ‡¦ | Grand Final | Eurovision 2024\u001b[39m\n",
      "\u001b[32m00:48:59.104:dazbo-yt-demos - INF: Downloading video alyona alyona _ Jerry Heil - Teresa _ Maria _LIVE_ _ Ukraine __ _ Grand Final _ Eurovision 2024.mp4 ...\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " â†³ |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:49:00.349:dazbo-yt-demos - INF: Creating audio...\u001b[39m\n",
      "/tmp/ipykernel_89553/3641805842.py:28: DeprecationWarning: The 'mp3' parameter is deprecated and will be removed in a future version. All audio will be downloaded as .m4a, the correct extension for the MPEG-4 AAC codec.\n",
      "  audio_stream.download(output_path=output_locn, filename=cleaned, mp3=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " â†³ |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:49:00.718:dazbo-yt-demos - INF: Done\u001b[39m\n",
      "\u001b[32m00:49:00.719:dazbo-yt-demos - INF: Downloads finished. See files in /mnt/c/Users/djl/localdev/python/youtube-and-video/src/notebooks/dazbo-yt-demos/output/pytubefix.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pytubefix import YouTube\n",
    "from pytubefix.cli import on_progress\n",
    "\n",
    "output_locn = f\"{locations.output_dir}/pytubefix\"\n",
    "\n",
    "def process_yt_videos():\n",
    "    for i, url in enumerate(urls):\n",
    "        logger.info(f\"Downloads progress: {i+1}/{len(urls)}\")\n",
    "\n",
    "        try:\n",
    "            yt = YouTube(url, on_progress_callback=on_progress)\n",
    "            logger.info(f\"Getting: {yt.title}\")\n",
    "            video_stream = yt.streams.get_highest_resolution()\n",
    "            if not video_stream:\n",
    "                raise Exception(\"Stream not available.\")\n",
    "            \n",
    "            # YouTube resource titles may contain special characters which \n",
    "            # can't be used when saving the file. So we need to clean the filename.\n",
    "            cleaned = clean_filename(yt.title)\n",
    "            \n",
    "            video_output = f\"{output_locn}/{cleaned}.mp4\"\n",
    "            logger.info(f\"Downloading video {cleaned}.mp4 ...\")\n",
    "            video_stream.download(output_path=output_locn, filename=f\"{cleaned}\")\n",
    "        \n",
    "            logger.info(f\"Creating audio...\")\n",
    "            audio_stream = yt.streams.get_audio_only()\n",
    "            audio_stream.download(output_path=output_locn, filename=cleaned, mp3=True)\n",
    "            \n",
    "            logger.info(\"Done\")\n",
    "            \n",
    "        except Exception as e:        \n",
    "            logger.error(f\"Error processing URL '{url}'.\")\n",
    "            logger.debug(f\"The cause was: {e}\") \n",
    "            \n",
    "    logger.info(f\"Downloads finished. See files in {output_locn}.\")\n",
    "    \n",
    "process_yt_videos()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Existing Transcripts from Videos\n",
    "\n",
    "Now I'm going to use the [youtube-transcript-api](https://github.com/jdepoix/youtube-transcript-api) to extract existing transcripts from YouTube videos. Not only will it return the transcript, but it can also be used to translate those to translate those transcripts into other languages.  So now I can download my Ukrainian song, and see both the Ukrainian transcript and the English translation. This is pretty awesome!\n",
    "\n",
    "However, some videos do not contain transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --no-cache-dir youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_transcript_api as yt_api\n",
    "from pytubefix import YouTube\n",
    "from pytubefix.cli import on_progress\n",
    "\n",
    "def get_transcripts():\n",
    "    \"\"\" Extract existing transcript data from videos \"\"\"\n",
    "    for url in urls:\n",
    "        try: # Just so we can get the video title\n",
    "            yt = YouTube(url, on_progress_callback=on_progress)\n",
    "        except Exception as e:        \n",
    "            logger.error(f\"Error processing URL '{url}'.\")\n",
    "            logger.debug(f\"The cause was: {e}\") \n",
    "            continue\n",
    "        \n",
    "        logger.info(f\"Processing '{yt.title}'...\")\n",
    "        video_id = get_video_id(url)\n",
    "        \n",
    "        try:\n",
    "            # By default, we get a list of 1: only get the preferred language transcript\n",
    "            transcript_list = yt_api.YouTubeTranscriptApi.list_transcripts(video_id)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unable to extract transcript for '{yt.title}'.\")\n",
    "            logger.debug(e)\n",
    "            continue\n",
    "        \n",
    "        # iterate over all available transcripts\n",
    "        for transcript in transcript_list:\n",
    "            # The Transcript object provides metadata properties. Here are some...\n",
    "            properties = {\n",
    "                \"video_id\": transcript.video_id,\n",
    "                \"language\": transcript.language,\n",
    "                \"language_code\": transcript.language_code,\n",
    "                \"is_generated\": transcript.is_generated,  # Whether it has been manually created or generated by YouTube\n",
    "                \"is_translatable\": transcript.is_translatable,  # Whether this transcript can be translated or not\n",
    "                \"translation_languages\": transcript.translation_languages,\n",
    "            }\n",
    "            \n",
    "            for prop, value in properties.items():\n",
    "                logger.info(f\"{prop}: {value}\")\n",
    "\n",
    "            # Fetch the actual transcript data\n",
    "            transcript_data = transcript.fetch() # returns a list of dicts\n",
    "            logger.info(f\"Raw transcript:\\n{transcript_data}\") \n",
    "            \n",
    "            processed_transcript = process_transcript(transcript_data)\n",
    "            logger.info(f\"Processed transcript:\\n{processed_transcript}\")\n",
    "            \n",
    "            # Translate to en if we can\n",
    "            if (transcript.language_code != \"en\" and \n",
    "                    transcript.is_translatable and \n",
    "                    any(lang['language_code'] == 'en' for lang in transcript.translation_languages)):\n",
    "                transcript_data = transcript.translate('en').fetch() # translate to en\n",
    "                processed_transcript = process_transcript(transcript_data)\n",
    "                logger.info(f\"Processed translated transcript:\\n{processed_transcript}\")\n",
    "\n",
    "def process_transcript(transcript_data):\n",
    "    \"\"\" Get all entries that are of type 'text' and NOT starting with [ \"\"\"\n",
    "    return \"\\n\".join([entry['text'] for entry in transcript_data \n",
    "                                     if entry['text'][0] != \"[\"])\n",
    "                \n",
    "get_transcripts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How cool is this!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Google Cloud Smarts\n",
    "\n",
    "Now we're going to leverage Google Cloud APIs. In order to leverage these Google services, you'll need to have first created a Google Cloud project.\n",
    "\n",
    "Then, in order to give your notebook access to the Google Cloud APIs, you broadly have three options:\n",
    "\n",
    "1. You can build and run your notebook locally.\n",
    "1. You can build and run your notebook in Google Colab.\n",
    "1. You can build and run your notebook in the Google Vertex AI Workbench environment.\n",
    "\n",
    "A bit more detail...\n",
    "\n",
    "### Local Notebook\n",
    "\n",
    "For local development you will need to:\n",
    "\n",
    "1. Have the Google Cloud `gcloud CLI` installed. See instructions [here](https://cloud.google.com/sdk/docs/install).\n",
    "1. Authenticate to your gcloud environment.\n",
    "1. Use [Application Default Credentials](https://cloud.google.com/docs/authentication/application-default-credentials) from within the notebook.\n",
    "\n",
    "The process looks like this:\n",
    "\n",
    "Code example:\n",
    "\n",
    "```python\n",
    "from google.auth import default\n",
    "!gcloud auth application-default login\n",
    "credentials, _ = default()\n",
    "\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"europe-west2\"\n",
    "\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "```\n",
    "\n",
    "### Google Colab\n",
    "\n",
    "The great thing about this approach is that Colab provides native integration to authenticate your user account and provide your Google project details to the Colab environment.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- No need to install Google `gcloud` locally.\n",
    "- Share notebooks using Google Drive. Google-drive based access control.\n",
    "- There are limitations for notebook size, and for notebook runtime instance size.\n",
    "\n",
    "Check out [this guide](https://github.com/GoogleCloudPlatform/devrel-demos/blob/main/other/colab/Using%20Google%20Cloud%20from%20Colab.ipynb).\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "import vertexai\n",
    "\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"europe-west2\"\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    # Authenticate user\n",
    "    auth.authenticate_user()\n",
    "    credentials, _ = google.auth.default()\n",
    "\n",
    "vertexai.init(\n",
    "    project=MY_PROJECT,\n",
    "    location=VERTEX_LOCATION,\n",
    "    credentials=credentials\n",
    ")\n",
    "```\n",
    "\n",
    "### Vertex AI Workbench\n",
    "\n",
    "[Vertex AI Workbench](https://cloud.google.com/vertex-ai/docs/workbench/introduction) is Google's managed enterprise Jupyter notebook hosting service. Is is fully-integrated with the Google Cloud and Vertex AI ecosystem.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- The JupyterLab environment is pre-installed.\n",
    "- Access control and sharing is managed by Google Cloud IAM, rather than Google Drive.\n",
    "- Because it is natively integrated with the Google Cloud environment, you don't need to provide any credentials or authenticate. You just need to provide Google project ID and region to any services that require this information. E.g.\n",
    "\n",
    "```python\n",
    "import vertexai\n",
    "\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"europe-west2\"\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:09:51.418:dazbo-yt-demos - INF: PROJECT_ID retrieved: video-smarts-442000\u001b[39m\n",
      "\u001b[32m00:09:51.419:dazbo-yt-demos - INF: REGION retrieved: europe-west2\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from getpass import getpass\n",
    "\n",
    "# Retrieve PROJECT_ID and other variables from any .env we can find\n",
    "try:\n",
    "    dc.get_envs_from_file()\n",
    "except ValueError as e:\n",
    "    logger.error(f\"Problem reading env file:\\n{e}\")\n",
    "\n",
    "env_vars = [\"PROJECT_ID\", \"REGION\"] # The vars we want to retrieve\n",
    "for env_var in env_vars:\n",
    "    if not os.getenv(env_var):\n",
    "        PROJECT_ID = '' # @param {type: \"string\"}\n",
    "        # If not retrieved from .env we'll need to input the value\n",
    "        os.environ[env_var] = getpass(f\"Enter {env_var}: \")\n",
    "\n",
    "    # Set Python variable of the same name as the env var, e.g. PROJECT_ID\n",
    "    globals()[env_var] = os.environ[env_var]\n",
    "    val = globals()[env_var]\n",
    "    logger.info(f\"{env_var} retrieved: {val}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear Environment Variables\n",
    "\n",
    "Only run the next cell if you want to manually clear the environment variables and then input new values. In this scenario, you'll also want to comment out any variables in your .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:09:45.875:dazbo-yt-demos - INF: Cleared environment variable: PROJECT_ID\u001b[39m\n",
      "\u001b[32m00:09:45.876:dazbo-yt-demos - INF: Cleared environment variable: REGION\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Optionally run this if we want to clear env vars\n",
    "for env_var in env_vars:\n",
    "    if env_var in os.environ:\n",
    "        del os.environ[env_var]\n",
    "        logger.info(f\"Cleared environment variable: {env_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-auth google-auth-oauthlib google-auth-httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're running Google Colab, authenticate\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "else:\n",
    "    # Be sure to set ADC if not running Vertex AI: !gcloud auth application-default login\n",
    "    pass\n",
    "\n",
    "from google.auth import default\n",
    "credentials, _ = default()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Transcription Usng the Video Intelligence API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-videointelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import videointelligence\n",
    "\n",
    "video_client = videointelligence.VideoIntelligenceServiceClient()\n",
    "features = [videointelligence.Feature.SPEECH_TRANSCRIPTION]\n",
    "\n",
    "config = videointelligence.SpeechTranscriptionConfig(\n",
    "    language_code=\"en-US\", enable_automatic_punctuation=True\n",
    ")\n",
    "video_context = videointelligence.VideoContext(speech_transcription_config=config)\n",
    "\n",
    "# Fortunately, this API natively supports mp4 without any conversion\n",
    "for video in Path(output_locn).glob(f'*.mp4'):\n",
    "    logger.info(f\"Processing {video}...\")\n",
    "\n",
    "    try:\n",
    "        with io.open(video, \"rb\") as file:\n",
    "            input_content = file.read()\n",
    "            \n",
    "        operation = video_client.annotate_video(\n",
    "            request={\n",
    "                \"features\": features,\n",
    "                \"input_content\": input_content,\n",
    "                \"video_context\": video_context,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        logger.info(\"Processing video for speech transcription.\")\n",
    "        result = operation.result(timeout=600)\n",
    "\n",
    "        # There is only one annotation_result since only one video is processed.\n",
    "        annotation_results = result.annotation_results[0]\n",
    "        for speech_transcription in annotation_results.speech_transcriptions:\n",
    "            # The number of alternatives for each transcription is limited by\n",
    "            # SpeechTranscriptionConfig.max_alternatives.\n",
    "            # Each alternative is a different possible transcription\n",
    "            # and has its own confidence score.\n",
    "            for alternative in speech_transcription.alternatives:\n",
    "                print(\"Alternative level information:\")\n",
    "\n",
    "                print(\"Transcript: {}\".format(alternative.transcript))\n",
    "                print(\"Confidence: {}\\n\".format(alternative.confidence))\n",
    "\n",
    "                print(\"Word level information:\")\n",
    "                for word_info in alternative.words:\n",
    "                    word = word_info.word\n",
    "                    start_time = word_info.start_time\n",
    "                    end_time = word_info.end_time\n",
    "                    print(\n",
    "                        \"\\t{}s - {}s: {}\".format(\n",
    "                            start_time.seconds + start_time.microseconds * 1e-6,\n",
    "                            end_time.seconds + end_time.microseconds * 1e-6,\n",
    "                            word,\n",
    "                        )\n",
    "                    )\n",
    "    except Exception as e:\n",
    "        logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI\n",
    "\n",
    "Let's integrate some Google Cloud Vertex AI smarts. Start by installing the **Google Cloud Vertex AI SDK for Python**. \n",
    "\n",
    "From [Introduction to the Vertex AI SDK for Python](https://cloud.google.com/vertex-ai/docs/python-sdk/use-vertex-ai-python-sdk#sdk-vs-client-library):\n",
    "\n",
    "When you install the Vertex AI SDK for Python (`google.cloud.aiplatform`), the Vertex AI Python client library (`google.cloud.aiplatform.gapic`) is also installed. The Vertex AI SDK and the Vertex AI Python client library provide similar functionality with different levels of granularity. The Vertex AI SDK operates at a higher level of abstraction than the client library and is suitable for most common data science workflows. If you need lower-level functionality, then use the Vertex AI Python client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Vertex AI SDK for Python and Vertex Generative AI SDK for Python\n",
    "%pip install --upgrade google-cloud-aiplatform \\\n",
    "                       google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform # Google Cloud Vertex AI SDK for Python\n",
    "# import vertexai   # Google Cloud Vertex Generative AI SDK for Python\n",
    "import google.generativeai as genai  # Google Gemini API (GenAI)\n",
    "from vertexai.generative_models import GenerativeModel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".yt-and-vid-conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
